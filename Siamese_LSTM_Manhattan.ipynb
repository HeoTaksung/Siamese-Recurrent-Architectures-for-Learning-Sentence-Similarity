{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1,2\"\n",
    "\n",
    "import tensorflow as tf\n",
    "from keras import layers\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def int_sentence(left_texts, right_texts, vocab_index):\n",
    "    left_int = []\n",
    "    right_int = []\n",
    "\n",
    "    for i in range(len(left_texts)):\n",
    "        left_etc = []\n",
    "        right_etc = []\n",
    "        for j in range(len(left_texts[i])):\n",
    "            left_etc.append(vocab_index[left_texts[i][j]])\n",
    "        for j in range(len(right_texts[i])):\n",
    "            right_etc.append(vocab_index[right_texts[i][j]])\n",
    "        left_int.append(left_etc)\n",
    "        right_int.append(right_etc)\n",
    "        \n",
    "    return left_int, right_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def manhattan_distance(left, right):\n",
    "    return K.exp(-K.sum(K.abs(left-right), axis=1, keepdims=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "All = open('file_name_1', 'r', encoding='utf-8-sig')   # train, cv, test가 모두 있는 파일\n",
    "train = open('file_name_2', 'r', encoding='utf-8-sig')\n",
    "cv = open('file_name_3', 'r', encoding='utf-8-sig')\n",
    "test = open('file_name_4', 'r', encoding='utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentence = []\n",
    "\n",
    "for line in All:\n",
    "    line = line.split('\\t')\n",
    "    all_sentence.append(line[0].split())\n",
    "    all_sentence.append(line[1].split())\n",
    "\n",
    "max_len = max([len(i) for i in all_sentence])\n",
    "\n",
    "vocab = set()\n",
    "for line in all_sentence:\n",
    "    for word in line:\n",
    "        vocab.add(word)\n",
    "\n",
    "vocab_size = len(vocab)+1\n",
    "\n",
    "vocab = sorted(list(vocab))\n",
    "\n",
    "vocab_index = {}\n",
    "for i in range(len(vocab)):\n",
    "    vocab_index[vocab[i]] = len(vocab_index)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Finished make embedding index ---\n",
      "Found 12178 word vectors.\n",
      "Created Embedded Matrix: 12178 word vectors.\n"
     ]
    }
   ],
   "source": [
    "embed_model_dir = 'embedding_pretrain_file'\n",
    "\n",
    "embedding_index = dict()\n",
    "\n",
    "with open(embed_model_dir, 'r', encoding='utf-8-sig') as f:\n",
    "    for line in f:\n",
    "        # Exception to first line in word2vec model.\n",
    "        if len(line.split()) == 2:\n",
    "            continue\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vectors = np.asarray(values[1:], dtype='float32')\n",
    "        embedding_index[word] = vectors\n",
    "    f.close()\n",
    "    print('--- Finished make embedding index ---')\n",
    "print('Found %s word vectors.' % len(embedding_index))\n",
    "\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "embedded_matrix = np.zeros((len(vocab_index)+1, EMBEDDING_DIM))\n",
    "embed_cnt = 0\n",
    "\n",
    "for word, i in vocab_index.items():\n",
    "    embedded_vector = embedding_index.get(word)\n",
    "    if embedded_vector is not None:\n",
    "        embedded_matrix[i] = embedded_vector\n",
    "        embed_cnt += 1\n",
    "\n",
    "print('Created Embedded Matrix: %s word vectors.' % embed_cnt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedded_layer = layers.Embedding(len(vocab_index)+1, EMBEDDING_DIM, \n",
    "    weights=[embedded_matrix], input_length=max_len, trainable=False)\n",
    "\n",
    "train_left_sen = []\n",
    "train_right_sen = []\n",
    "train_label = []\n",
    "\n",
    "for line in train:\n",
    "    line = line.split('\\t')\n",
    "    train_left_sen.append(line[0].strip().split())\n",
    "    train_right_sen.append(line[1].strip().split())\n",
    "    train_label.append(float(line[2].strip()))\n",
    "\n",
    "cv_left_sen = []\n",
    "cv_right_sen = []\n",
    "cv_label = []\n",
    "\n",
    "for line in cv:\n",
    "    line = line.split('\\t')\n",
    "    cv_left_sen.append(line[0].strip().split())\n",
    "    cv_right_sen.append(line[1].strip().split())\n",
    "    cv_label.append(float(line[2].strip()))\n",
    "\n",
    "test_left_sen = []\n",
    "test_right_sen = []\n",
    "test_label = []\n",
    "\n",
    "for line in test:\n",
    "    line = line.split('\\t')\n",
    "    test_left_sen.append(line[0].strip().split())\n",
    "    test_right_sen.append(line[1].strip().split())\n",
    "    test_label.append(float(line[2].strip()))\n",
    "\n",
    "train_left_int, train_right_int = int_sentence(train_left_sen, train_right_sen, vocab_index)\n",
    "cv_left_int, cv_right_int = int_sentence(cv_left_sen, cv_right_sen, vocab_index)\n",
    "test_left_int, test_right_int = int_sentence(test_left_sen, test_right_sen, vocab_index)\n",
    "\n",
    "train_left = pad_sequences(train_left_int, padding='post', maxlen=max_len)\n",
    "train_right = pad_sequences(train_right_int, padding='post', maxlen=max_len)\n",
    "\n",
    "cv_left = pad_sequences(cv_left_int, padding='post', maxlen=max_len)\n",
    "cv_right = pad_sequences(cv_right_int, padding='post', maxlen=max_len)\n",
    "\n",
    "test_left = pad_sequences(test_left_int, padding='post', maxlen=max_len)\n",
    "test_right = pad_sequences(test_right_int, padding='post', maxlen=max_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 69)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            (None, 69)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 69, 300)      3849300     input_1[0][0]                    \n",
      "                                                                 input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   (None, 50)           70200       embedding_1[0][0]                \n",
      "                                                                 embedding_1[1][0]                \n",
      "__________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)               (None, 1)            0           lstm_1[0][0]                     \n",
      "                                                                 lstm_1[1][0]                     \n",
      "==================================================================================================\n",
      "Total params: 3,919,500\n",
      "Trainable params: 70,200\n",
      "Non-trainable params: 3,849,300\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "left_input = layers.Input(shape=(max_len,))\n",
    "right_input = layers.Input(shape=(max_len,))\n",
    "\n",
    "left_emd = embedded_layer(left_input)\n",
    "right_emd = embedded_layer(right_input)\n",
    "\n",
    "lstm = layers.LSTM(50)\n",
    "\n",
    "left_lstm = lstm(left_emd)\n",
    "right_lstm = lstm(right_emd)\n",
    "\n",
    "distance = layers.Lambda(function=lambda x: manhattan_distance(x[0], x[1]),output_shape=lambda x: (x[0][0], 1))([left_lstm, right_lstm])\n",
    "\n",
    "optimizer = keras.optimizers.Adadelta(clipnorm=1.25)  # paper optimizers\n",
    "\n",
    "model = Model(inputs=[left_input, right_input], outputs=[distance])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.utils.multi_gpu_model(model, gpus=3)\n",
    "model.compile(loss=\"mean_squared_error\", optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "es = keras.callbacks.EarlyStopping(monitor='val_loss',mode='min', verbose=1, patience=3, restore_best_weights=True)\n",
    "\n",
    "model_fit = model.fit([train_left, train_right], [train_label], batch_size=64, epochs=100, validation_data=([cv_left, cv_right], [cv_label]), callbacks=[es])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluation = model.evaluate([test_left, test_right], [test_label])\n",
    "\n",
    "print('Accuracy: '+str(evaluation[1]))\n",
    "print('Loss: '+str(evaluation[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
